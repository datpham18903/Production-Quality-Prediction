{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1122579,"sourceType":"datasetVersion","datasetId":630763}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install liac-arff","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport arff\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')\n\nfrom datetime import datetime, timedelta\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 1000)\n\nnp.random.seed(42)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Data Loading and Inspection","metadata":{}},{"cell_type":"code","source":"X = pd.read_csv(\"/kaggle/input/production-quality/data_X.csv\")\nY = pd.read_csv(\"/kaggle/input/production-quality/data_Y.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/production-quality/sample_submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Data_X shape:\", X.shape)\nprint(\"Data_Y shape:\", Y.shape)\nprint(\"Sample Submission shape:\", submission.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Data_X columns:\\n\", X.columns)\nprint(\"Data_Y columns:\\n\", Y.columns)\nprint(\"Sample Submission columns:\\n\", submission.columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Y.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Y.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_counts = X.isna().sum().to_frame(name='missing_counts')\nprint(missing_counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_counts = Y.isna().sum().to_frame(name='missing_counts')\nprint(missing_counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_counts = submission.isna().sum().to_frame(name='missing_counts')\nprint(missing_counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Data Cleaning","metadata":{}},{"cell_type":"code","source":"X_clean = X.copy()\nY_clean = Y.copy()\nsubmission_clean = submission.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_cols = X_clean.select_dtypes(include=['int64', 'float64']).columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, len(numerical_cols) * 4))\n\nfor i, col in enumerate(numerical_cols):\n    plt.subplot(len(numerical_cols), 3, i*3 + 1)\n    sns.histplot(X_clean[col], kde=True, color=\"dodgerblue\")\n    plt.title(f'Distribution of {col}')\n    \n    plt.subplot(len(numerical_cols), 3, i*3 + 2)\n    sns.boxplot(x=X_clean[col], color=\"dodgerblue\")\n    plt.title(f'Boxplot of {col}')\n    \n    plt.subplot(len(numerical_cols), 3, i*3 + 3)\n    res = stats.probplot(X_clean[col].dropna(), plot=plt)\n    plt.plot(res[0][0], res[0][1], 'o', color=\"dodgerblue\")\n    plt.title(f'Q-Q Plot of {col}')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cap_outliers(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    \n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    outliers = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n    print(f\"Found {outliers} outliers in {column} ({outliers/len(df)*100:.2f}%)\")\n    \n    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n    \n    return df\n\nfor col in numerical_cols:\n    if X_clean[col].nunique() > 10:\n        X_clean = cap_outliers(X_clean, col)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, len(numerical_cols) * 4))\n\nfor i, col in enumerate(numerical_cols):\n    plt.subplot(len(numerical_cols), 3, i*3 + 1)\n    sns.histplot(X_clean[col], kde=True, color=\"dodgerblue\")\n    plt.title(f'Distribution of {col}')\n    \n    plt.subplot(len(numerical_cols), 3, i*3 + 2)\n    sns.boxplot(x=X_clean[col], color=\"dodgerblue\")\n    plt.title(f'Boxplot of {col}')\n    \n    plt.subplot(len(numerical_cols), 3, i*3 + 3)\n    res = stats.probplot(X_clean[col].dropna(), plot=plt)\n    plt.plot(res[0][0], res[0][1], 'o', color=\"dodgerblue\")\n    plt.title(f'Q-Q Plot of {col}')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_clean['date_time'] = pd.to_datetime(X_clean['date_time'])\nY_clean['date_time'] = pd.to_datetime(Y_clean['date_time'])\nsubmission_clean['date_time'] = pd.to_datetime(submission_clean['date_time'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"X_clean date range: {X_clean['date_time'].min()} to {X_clean['date_time'].max()}\")\nprint(f\"Y_clean date range: {Y_clean['date_time'].min()} to {Y_clean['date_time'].max()}\")\nprint(f\"submission_clean date range: {submission_clean['date_time'].min()} to {submission_clean['date_time'].max()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_clean['hour'] = X_clean['date_time'].dt.floor('H')\n\nX_hourly = X_clean.groupby('hour').agg({\n    col: 'mean' for col in numerical_cols\n}).reset_index()\n\nX_hourly = X_hourly.rename(columns={'hour': 'date_time'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Y_clean['hour'] = Y_clean['date_time'].dt.floor('H')\n\ncommon_hours = set(Y_clean['hour'].unique())\nX_hourly_aligned = X_hourly[X_hourly['date_time'].isin(common_hours)]\n\nY_hourly = Y_clean.drop_duplicates(subset=['hour']).copy()\nY_hourly['date_time'] = Y_hourly['hour']\nY_hourly = Y_hourly.drop('hour', axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"X_hourly shape before alignment: {X_hourly.shape}\")\nprint(f\"X_hourly shape after alignment: {X_hourly_aligned.shape}\")\nprint(f\"Y_hourly shape: {Y_hourly.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_hourly = X_hourly_aligned.reset_index().drop(\"index\", axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"X_hourly date range: {X_hourly['date_time'].min()} to {X_hourly['date_time'].max()}\")\nprint(f\"Y_hourly date range: {Y_hourly['date_time'].min()} to {Y_hourly['date_time'].max()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **3. Exploratory Data Analysis (EDA)**","metadata":{}},{"cell_type":"code","source":"numerical_cols = X_hourly.select_dtypes(include=['int64', 'float64']).columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_summary = X_hourly[numerical_cols].describe()[1:].T\nnumerical_summary['range'] = numerical_summary['max'] - numerical_summary['min']\nnumerical_summary['coefficient_of_variation'] = numerical_summary['std'] / numerical_summary['mean'] * 100\nnumerical_summary = numerical_summary.style.background_gradient(cmap='coolwarm')\ndisplay(numerical_summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if 'quality' in Y_hourly.columns:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(Y_hourly['quality'], kde=True, color='dodgerblue')\n    plt.title('Distribution of Target Variable (Quality)')\n    plt.show()\n    \n    print(\"Summary statistics of target variable:\")\n    display(Y_hourly['quality'].describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation analysis\ncorrelation_matrix = X_hourly[numerical_cols].corr()\n\nplt.figure(figsize=(12, 10))\nmask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\nsns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', \n            center=0, square=True, linewidths=.5)\nplt.title('Correlation Matrix of Numerical Features')\nplt.tight_layout()\nplt.show()\n\n# Identify highly correlated features\nthreshold = 0.7\nhigh_corr_features = set()\n\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > threshold:\n            featurename = correlation_matrix.columns[i]\n            high_corr_features.add(featurename)\n            \nprint(f\"Highly correlated features (|corr| > {threshold}):\")\nprint(high_corr_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4. Feature Engineering**","metadata":{}},{"cell_type":"code","source":"X_engineered = X_hourly.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_engineered['hour'] = X_engineered['date_time'].dt.hour\nX_engineered['dayofweek'] = X_engineered['date_time'].dt.dayofweek","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_engineered['hour_sin'] = np.sin(2 * np.pi * X_engineered['hour']/24)\nX_engineered['hour_cos'] = np.cos(2 * np.pi * X_engineered['hour']/24)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_engineered['dayofweek_sin'] = np.sin(2 * np.pi * X_engineered['dayofweek']/7)\nX_engineered['dayofweek_cos'] = np.cos(2 * np.pi * X_engineered['dayofweek']/7)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_engineered.drop(columns=[\"hour\", \"dayofweek\"], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Feature engineered data shape: {X_engineered.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **5. Train-Test Split**","metadata":{}},{"cell_type":"code","source":"print(\"X_hourly shape:\", X_engineered.shape)\nprint(\"Y_hourly shape:\", Y_hourly.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_data = pd.merge(\n    X_engineered,\n    Y_hourly[[\"date_time\", \"quality\"]],\n    on=\"date_time\",\n    how=\"inner\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Training data shape after merging: {training_data.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_data = training_data.drop([\"quality\", \"date_time\"], axis=1)\ny_data = training_data[\"quality\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\nprint(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Standardization","metadata":{}},{"cell_type":"code","source":"numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_scaled = X_train.copy()\nX_test_scaled = X_test.copy()\n\nX_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\nX_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Summary statistics after scaling (training set):\")\ndisplay(X_train_scaled[numerical_cols].describe()[1:].T.style.background_gradient(cmap='coolwarm'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **7. Save Processed Data**","metadata":{}},{"cell_type":"code","source":"output_dir = '/kaggle/working/preprocessed'\nos.makedirs(output_dir, exist_ok=True)\n\ntrain_data = X_train_scaled.copy()\ntrain_data['quality'] = y_train.values\n\ntest_data = X_test_scaled.copy()\ntest_data['quality'] = y_test.values\n\ndef dataframe_to_arff(df, relation_name):\n    df = df.copy()\n    \n    if 'date_time' in df.columns:\n        df = df.drop('date_time', axis=1)\n    \n    attributes = []\n    for col in df.columns:\n        attributes.append((col, 'NUMERIC'))\n    \n    data = df.values.tolist()\n    \n    arff_dict = {\n        'relation': relation_name,\n        'attributes': attributes,\n        'data': data\n    }\n    \n    return arff_dict\n\nprint(\"Training data columns:\", train_data.columns.tolist())\nprint(\"Test data columns:\", test_data.columns.tolist())\n\ntrain_arff = dataframe_to_arff(train_data, 'quality_prediction_train')\nwith open(f'{output_dir}/train_data.arff', 'w') as f:\n    f.write(arff.dumps(train_arff))\n\ntest_arff = dataframe_to_arff(test_data, 'quality_prediction_test')\nwith open(f'{output_dir}/test_data.arff', 'w') as f:\n    f.write(arff.dumps(test_arff))\n\nprint(f\"ARFF files created successfully:\")\nprint(f\"- {output_dir}/train_data.arff\")\nprint(f\"- {output_dir}/test_data.arff\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.shape()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.shape()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}